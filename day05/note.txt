一、使用终端
	Scrapy终端仅仅是一个普通的Python终端(或IPython )。其提供了一些额外的快捷方式。
	1、可用的快捷命令(shortcut)
		shelp() - 打印可用对象及快捷命令的帮助列表
		fetch(request_or_url) - 根据给定的请求(request)或URL获取一个新的response，并更新相关的对象
		view(response) - 在本机的浏览器打开给定的response。 其会在response的body中添加一个 <base> tag，使得外部链接(例如图片及css)能正确显示。注意，该操作会在本地创建一个临时文件，且该文件不会被自动删除。
	2、可用的Scrapy对象
		Scrapy终端根据下载的页面会自动创建一些方便使用的对象，例如Response对象及Selector对象(对HTML及XML内容)。
		这些对象有:
		crawler - 当前 Crawler 对象.
		spider - 处理URL的spider。 对当前URL没有处理的Spider时则为一个Spider对象。
		request - 最近获取到的页面的 Request 对象。您可以使用replace()修改该request。或者使用fetch 快捷方式来获取新的request。
		response - 包含最近获取到的页面的 Response 对象。
		sel - 根据最近获取到的response构建的 Selector 对象。
		settings - 当前的 Scrapy settings

二、spider类中使用shell
	可以通过scrapy.shell.inspect_response实现在spider中调用shell
	注意: 由于该终端阻塞Scrapy引擎，在这个终端中不能使用fetch快捷命令(shortcut)。 当您离开终端时，spider会从其停下的地方恢复爬取。

三、Item Pipeline
	每个item pipeline组件(有时称之为“Item Pipeline”)是实现了简单方法的Python类。他们接收到Item并通过它执行一些行为，同时也决定此Item是否继续通过pipeline，或是被丢弃而不再进行处理。
	以下是item pipeline的一些典型应用：
	1、清理HTML数据
	2、验证爬取的数据(检查item包含某些字段)
	3、查重(并丢弃)
	4、将爬取结果保存到数据库中

	如何编写自定义的pipeline:
		每个item pipiline组件是一个独立的Python类，同时必须实现以下方法:
		process_item(self, item, spider)
			每个item pipeline组件都需要调用该方法，这个方法必须返回一个具有数据的dict，或是Item(或任何继承类)对象， 或是抛出DropItem异常，被丢弃的item将不会被之后的pipeline组件所处理。
			参数:	
			item (Item 对象或者一个dict) – 被爬取的item
			spider (Spider 对象) – 爬取该item的spider
	附加实现的方法：
		open_spider(self,spider)
		当spider被开启时，这个方法被调用。
		参数:spider(Spider对象) – 被开启的spider

		close_spider(self, spider)
		当spider被关闭时，这个方法被调用
		参数:spider (Spider 对象) – 被关闭的spider
		
		from_crawler(cls, crawler)

		参数:crawler (Crawler object) – crawler that uses this pipeline
	3.1 如何将items写入Moongodb数据库
		对于MongoDB的address和database名，要在Scrapy的setting.py中进行配置，其中MongoDB的集合名会由item类来确定。

	3.2 启用一个Item Pipeline组件
		为了启用一个Item Pipeline组件，你必须将它的类添加到setting.py中的ITEM_PIPELINES配置，就像下面这个例子:
		ITEM_PIPELINES = {
		    'myproject.pipelines.PricePipeline': 300,
		    'myproject.pipelines.JsonWriterPipeline': 800,
		}
		分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序运行，通过pipeline(管道)，通常将这些数字定义在0-1000范围内。



